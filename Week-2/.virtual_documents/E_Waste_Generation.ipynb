!pip install -q tensorflow gradio pillow

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.applications import EfficientNetV2B0
from tensorflow.keras.applications.efficientnet_v2 import preprocess_input
import matplotlib.pyplot as plt
import numpy as np
import os
import zipfile
from google.colab import files
from PIL import Image
import gradio as gr
import datetime
import warnings
import shutil
warnings.filterwarnings('ignore')

print("E-Waste Classification Model")

#Step 1: Upload and Unzip Dataset
def setup_dataset():
    print("üìÅ Please upload a ZIP file containing e-waste image folders (by class name)")
    try:
        uploaded = files.upload()
        if not uploaded:
            raise ValueError("No file uploaded!")

        zip_path = list(uploaded.keys())[0]
        unzip_path = "/content/ewaste_dataset"

        if os.path.exists(unzip_path):
            shutil.rmtree(unzip_path)
        os.makedirs(unzip_path, exist_ok=True)

        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(unzip_path)

        contents = os.listdir(unzip_path)
        if len(contents) == 1 and os.path.isdir(os.path.join(unzip_path, contents[0])):
            nested_path = os.path.join(unzip_path, contents[0])
            for item in os.listdir(nested_path):
                shutil.move(os.path.join(nested_path, item), os.path.join(unzip_path, item))
            os.rmdir(nested_path)

        class_folders = [d for d in os.listdir(unzip_path) if os.path.isdir(os.path.join(unzip_path, d))]
        if len(class_folders) < 2:
            raise ValueError("At least 2 class folders are required.")

        total_images = sum([len([f for f in os.listdir(os.path.join(unzip_path, d)) if f.lower().endswith(('jpg', 'jpeg', 'png'))]) for d in class_folders])
        if total_images < 10:
            raise ValueError("Dataset must contain at least 10 images.")

        return unzip_path, class_folders
    except Exception as e:
        print(f"‚ùå Error setting up dataset: {e}")
        raise

#Step 2: Prepare Dataset
def prepare_datasets(unzip_path):
    total_images = sum([len([f for f in os.listdir(os.path.join(unzip_path, d))
                           if f.lower().endswith(('jpg', 'jpeg', 'png'))])
                       for d in os.listdir(unzip_path) if os.path.isdir(os.path.join(unzip_path, d))])

    batch_size = min(32, max(4, total_images // 10))
    img_height, img_width = 224, 224

    data_augmentation = tf.keras.Sequential([
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.05),
        layers.RandomZoom(0.05),
        layers.RandomContrast(0.05)
    ])

    val_split = 0.2 if total_images > 50 else 0.1
    train_ds = tf.keras.utils.image_dataset_from_directory(
        unzip_path,
        validation_split=val_split,
        subset="training",
        seed=42,
        image_size=(img_height, img_width),
        batch_size=batch_size
    )
    val_ds = tf.keras.utils.image_dataset_from_directory(
        unzip_path,
        validation_split=val_split,
        subset="validation",
        seed=42,
        image_size=(img_height, img_width),
        batch_size=batch_size
    )

    class_names = train_ds.class_names
    num_classes = len(class_names)

    AUTOTUNE = tf.data.AUTOTUNE
    train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)
    train_ds = train_ds.cache().shuffle(min(1000, total_images)).prefetch(buffer_size=AUTOTUNE)
    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

    return train_ds, val_ds, class_names, num_classes, batch_size, img_height, img_width

#Step 3: Build Model
def build_model(num_classes, img_height, img_width):
    base_model = EfficientNetV2B0(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3))
    base_model.trainable = True
    fine_tune_at = len(base_model.layers) - 30
    for layer in base_model.layers[:fine_tune_at]:
        layer.trainable = False

    inputs = tf.keras.Input(shape=(img_height, img_width, 3))
    x = preprocess_input(inputs)
    x = base_model(x, training=True)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.3)(x)

    if num_classes == 2:
        outputs = layers.Dense(1, activation='sigmoid')(x)
        loss_fn = 'binary_crossentropy'
    else:
        outputs = layers.Dense(num_classes, activation='softmax')(x)
        loss_fn = 'sparse_categorical_crossentropy'

    model = models.Model(inputs, outputs)
    model.compile(
        optimizer=tf.keras.optimizers.AdamW(learning_rate=5e-5, weight_decay=1e-5),
        loss=loss_fn,
        metrics=['accuracy']
    )
    return model







#---In Progess for better Improvements in Training Machine---

#Step 4: Train Model
def train_model(model, train_ds, val_ds):
    model_dir = f"/content/model_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
    os.makedirs(model_dir, exist_ok=True)

    callbacks_list = [
        callbacks.ModelCheckpoint(f"{model_dir}/best_model.h5", save_best_only=True, monitor='val_accuracy', mode='max'),
        callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True),
        callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=4, min_lr=1e-7)
    ]

    epochs = min(30, max(10, len(list(train_ds)) * 2))
    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=callbacks_list)
    model.save(f"{model_dir}/final_model", save_format='tf')
    return history, model_dir

#Step 5: Plot History
def plot_history(history):
    import matplotlib.pyplot as plt
    plt.figure(figsize=(14, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Acc')
    plt.plot(history.history['val_accuracy'], label='Val Acc')
    plt.title("Accuracy")
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title("Loss")
    plt.legend()
    plt.show()

#Step 6: Gradio Interface
def create_gradio_interface(model_dir, class_names, num_classes, img_width, img_height):
    model = tf.keras.models.load_model(f"{model_dir}/final_model")

    def predict(image):
        if image is None:
            return "Upload an image."
        img = image.convert("RGB").resize((img_width, img_height))
        img_array = tf.keras.utils.img_to_array(img)
        img_array = tf.expand_dims(img_array, 0)
        img_array = preprocess_input(img_array)
        predictions = model.predict(img_array, verbose=0)

        if num_classes == 2:
            score = float(predictions[0][0])
            return {
                class_names[1]: round(score, 4),
                class_names[0]: round(1 - score, 4)
            }
        else:
            probs = tf.nn.softmax(predictions[0]).numpy()
            return {class_names[i]: round(float(probs[i]), 4) for i in range(num_classes)}

    interface = gr.Interface(
        fn=predict,
        inputs=gr.Image(type="pil", label="Upload Image"),
        outputs=gr.Label(num_top_classes=num_classes),
        title="E-Waste Classifier",
        description=f"Upload an e-waste image to classify it among: {', '.join(class_names)}",
        theme=gr.themes.Soft()
    )
    return interface

#Main
def main():
    unzip_path, class_folders = setup_dataset()
    train_ds, val_ds, class_names, num_classes, batch_size, img_height, img_width = prepare_datasets(unzip_path)
    model = build_model(num_classes, img_height, img_width)
    history, model_dir = train_model(model, train_ds, val_ds)
    plot_history(history)
    interface = create_gradio_interface(model_dir, class_names, num_classes, img_width, img_height)
    interface.launch(share=True)

if __name__ == "__main__":
    main()







